# HPCâ€‘Courseâ€‘Examples
This repo is deliberately structured so that each practical topic lives in its own folder, includes a short tutorial, readyâ€‘toâ€‘run scripts.  

---  

## ğŸ“š  Table of Contents
1. [Course Overview](#course-overview)  
2. [Repository Layout](#repository-layout)  
3. [Prerequisites & Setup](#prerequisites--setup)  
4. [Running the Labs](#running-the-labs)  
5. [Example Topics & Scripts](#example-topics--scripts)  
   - Slurm basics & jobâ€‘viewer script  
   - Submitting & monitoring a real Slurm job  
   - `nvidiaâ€‘smi` cheatâ€‘sheet  
   - Querying Lustre filesystem info  
   - GPUâ€‘aware vs nonâ€‘aware MPI bandwidth demo  
6. [Assessments & Lab Manuals](#assessments--lab-manuals)  
7. [Running the Full Environment (Docker / Conda)](#running-the-full-environment)  
8. [Contributing & Issues](#contributing--issues)  
9. [License](#license)  

---  

## Course Overview
This repository accompanies an **HPC fundamentals course** (lecture + lab).  
Students will learn to:

* Submit and monitor jobs on a Slurm scheduler.  
* Parse Slurm accounting records and extract useful information programmatically.  
* Query GPU health and topology with `nvidia-smi`.  
* Inspect Lustre storage characteristics.  
* Compare GPUâ€‘aware and GPUâ€‘nonâ€‘aware MPI bandwidth on real hardware.  

All material is provided as **selfâ€‘contained examples** that can be explored on a single workstation or on an actual cluster.

---  

## Repository Layout
```
hpc-course/
â”‚
â”œâ”€ .github/                # GitHub Actions CI workflows
â”œâ”€ .vscode/                # optional VSâ€¯Code settings
â”œâ”€ docs/
â”‚   â”œâ”€ presentation.pdf    # full slide deck for the class
â”‚   â””â”€ handouts/grading_rubric.pdf
â”œâ”€ examples/
â”‚   â”œâ”€ 01_slurm_basics/
â”‚   â”‚   â””â”€ slurm_job_viewer/   (Python script + sample data)
â”‚   â”œâ”€ 02_slurm_example_job/
â”‚   â”‚   â”œâ”€ sbatch_script.slurm
â”‚   â”‚   â””â”€ README.md
â”‚   â”œâ”€ 03_nvidia_smi/
â”‚   â”‚   â””â”€ nvidia_smi_examples.sh
â”‚   â”œâ”€ 04_lustre_info/
â”‚   â”‚   â””â”€ lustre_info.sh
â”‚   â””â”€ 05_gpu_bandwidth/
â”‚       â”œâ”€ gpu_aware_mpi/
â”‚       â”‚   â”œâ”€â”€ Makefile
â”‚       â”‚   â””â”€ mpi_naive.c
â”‚       â””â”€ gpu_non_aware_mpi/
â”‚           â”œâ”€â”€ Makefile
â”‚           â””â”€ mpi_naive.c
â”œâ”€ scripts/
â”‚   â””â”€ generate_examples.sh
â”œâ”€ LICENSE
â”œâ”€ README.md               # â† you are here
â””â”€ .gitignore
```

---  

## Prerequisites & Setup
| Requirement | Reason |
|------------|--------|
| **Linux | All scripts assume a Unixâ€‘like shell. |
| **Pythonâ€¯3.9+** | Needed for the Slurmâ€‘viewer script and helper utilities. |
| **Slurm client** (`scontrol`, `squeue`, `sbatch`) | To generate real job accounting data. |
| **CUDA Toolkit** (if GPU labs are used) | Provides `nvidia-smi` and the CUDA runtime for MPI builds. |
| **OpenMPI** (or MPICH) built with CUDA support | For the GPUâ€‘aware MPI demos. |

### Quick environment creation (conda)
```bash
# Clone the repo first
git clone https://github.com/your-org/hpc-course.git
cd hpc-course

```

The environment installs:
* Python, `pip`
* `openmpi` (CPUâ€‘only build) **and** `openmpi-cuda` (GPUâ€‘aware optional)

---  

## Running the Labs

### 1ï¸âƒ£ Slurm Basics (Labâ€¯1)
```bash
cd examples/01_slurm_basics
./slurm_job_viewer.py           # interactive mode
#   â†’ enter a JobId (e.g. 12345) and select categories
```

*The script parses `scontrol show jobid` output and prints it in three labeled blocks.*  

A tiny sample file (`sample_slurm_data.txt`) is provided for testing on a machine **without** an active Slurm cluster.

### 2ï¸âƒ£ Submitting a Real Job (Labâ€¯2)
```bash
cd examples/02_slurm_example_job
sbatch sbatch_script.slurm      # submit
squeue -u $USER                 # view pending/running jobs
scontrol show jobid <JobId>    # detailed accounting info
```

The job runs a short `sleep 30` + `echo "Hello"` script; you can replace it with any executable.

### 3ï¸âƒ£ `nvidia-smi` Cheatâ€‘Sheet (Labâ€¯3)
```bash
cd examples/03_nvidia_smi
./nvidia_smi_examples.sh
```
*All commands are annotated with comments explaining what they do.*

### 4ï¸âƒ£ Lustre Filesystem Inspection (Labâ€¯4)
```bash
cd examples/04_lustre_info
./lustre_info.sh
```
*Shows capacity, quotas, and basic diagnostics on a Lustre mount point.*

### 5ï¸âƒ£ GPU Bandwidth Comparison (Labâ€¯5)
```bash
cd examples/05_gpu_bandwidth/gpu_aware_mpi
make                         # builds `gpu_aware_mpi`
cd ../gpu_non_aware_mpi
make                         # builds `non_aware_mpi`

# Run a sample measurement (matrix size 4096)
sbatch job_gpu_aware_mpi 
sbatch job_non_aware_mpi 
```

*The two binaries measure the bandwidth of a transferring data between two GPUs.

A Python helper (`plot_bw.py`) can be used to generate a bandwidthâ€‘vsâ€‘size plot.

---  

## Example Topics & Scripts  

### ğŸ“‚ `01_slurm_basics/slurm_job_viewer/`
* **`slurm_job_viewer.py`** â€“ parses `scontrol show jobid` output and prints three categorized blocks (General, Resources, Location).  
* **`sample_slurm_data.txt`** â€“ a tiny dump you can feed to the script when no running job exists.  
* **`README.md`** â€“ stepâ€‘byâ€‘step walkthrough and suggested exercises.

### ğŸ“‚ `02_slurm_example_job/`
* **`example_job.sh`** â€“ simple workload that prints â€œHelloâ€ and sleeps.  
* **`sbatch_script.slurm`** â€“ a minimal batch script that requests 2 CPUs, 1â€¯GB memory, and a 5â€‘minute wallâ€‘time limit.  
* **`README.md`** â€“ explains submission, monitoring with `squeue`/`scontrol`, and how to inspect the jobâ€™s resource usage.

### ğŸ“‚ `03_nvidia_smi/`
* **`nvidia_smi_examples.sh`** â€“ a collection of practical `nvidiaâ€‘smi` commands (querying, monitoring, topology, memory limits, etc.).  
* **`README.md`** â€” explains each command and typical useâ€‘cases.

### ğŸ“‚ `04_lustre_info/`
* **`lustre_info.sh`** â€“ runs a handful of `lfs` commands (`df`, `quota`, `summary`, `check`).  
* **`README.md`** â€” quick reference for gathering storage metrics on a Lustre filesystem.

### ğŸ“‚ `05_gpu_bandwidth/`
* Two subâ€‘folders:
  * **`gpu_aware_mpi/`** â€“ MPI built with `-DGPU_AWARE`; reads directly from GPU memory via CUDAâ€‘aware MPI.  
  * **`gpu_non_aware_mpi/`** â€“ same code compiled **without** the flag (forces data to be copied to host memory).  
* Each side contains a `Makefile` and the source (`mpi_naive.c`).  
* **`README.md`** â€” compilation instructions, sample run commands, and how to plot the results with `plot_bw.py`.

---  

## License
The code in this repository is released under the **MIT License** (see the `LICENSE` file).  
The PDFs and other nonâ€‘code assets are covered by the same license unless otherwise noted.

---  

## ğŸ“¬  Contact
* **Instructor:** Dr.â€¯Hicham AGueny â€“ hicham.agueny@uib.no  

---  

*Happy hacking, and enjoy exploring HPC!* 
