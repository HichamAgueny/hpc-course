# GPUâ€‘aware MPI Bandwidth Test  

---  

## Purpose  

This repository contains a readyâ€‘toâ€‘run MPI program that measures the **bandwidth of data transfer between two GPUs** on the **Olivia** cluster.  

| Scenario | Communication mode | What happens under the hood |
|----------|-------------------|-----------------------------|
| **IPC enabled** | Direct **GPUâ€‘toâ€‘GPU** transfer via **NVlink* (Intraâ€‘node). | Data never touches the host memory â€“ the fastest possible path. |
| **IPC disabled** | Hostâ€‘mediated transfer (MPI â†’ host â†’ GPU). | Slower, useful as a baseline or when IPC is not available. |

Both the source code **has already been compiled** for the Olivia compute nodes, and two Slurm submission scripts are provided to run the two cases:

* `script_gpuaware_IPC_enabled_Olivia.sh`â€ƒâ€ƒÂ Â â†’ IPC **enabled**  
* `script_gpuaware_IPC_disabled_Olivia.sh`â€ƒ  â†’ IPC **disabled**

Your only task is to **submit** the appropriate script with `sbatch` and compare the printed bandwidth results.

### Case I: IPC **enabled**  
  
```bash
cd gpu_aware_mpi
sbatch script_gpuaware_IPC_enabled_Olivia.sh
cat slurm/slurm_olivia_IPC_enabled*.out
```
### Case II: IPC **disabled**  

```bash
sbatch script_gpuaware_IPC_disabled_Olivia.sh
cat slurm/slurm_olivia_IPC_disabled*.out
```
---  

## ğŸ“‚â€¯Folder layout  

```
gpu_aware_mpi/
â”‚
â”œâ”€ gpuaware_bandwidth.f90   â† MPI-OpenACC source (already compiled)
â”œâ”€ compile.sh               â† compile script (already executed)
â”‚
â”œâ”€ script_gpuaware_IPC_enabled_Olivia.sh    â† slurm script for the case of IPC enabled
â”œâ”€ script_gpuaware_IPC_disabled_Olivia.sh   â† slurm script for the case of IPC disbaled
â”‚
â””â”€ slurm/                   â† (optional) slurm output
```

---  

## How to run  

### 1ï¸âƒ£ Submit the *IPCâ€‘enabled* job  

```bash
cd gpu_aware_mpi
sbatch script_gpuaware_IPC_enabled_Olivia.sh
```

The script will:

1. Load any required modules (e.g., `CrayEnv`, `cuda/12.6`, `cray-mpich`).  
2. Launch the preâ€‘compiled executable `gpuaware.olivia.exe` with the `srun` launcher.
3. Print a table like:

```
...
...
Size (B):    134217728 | Time/transfer (s):     0.001021 | Bandwidth (GB/s):      131.400
Size (B):    268435456 | Time/transfer (s):     0.002029 | Bandwidth (GB/s):      132.325
Size (B):    536870912 | Time/transfer (s):     0.004043 | Bandwidth (GB/s):      132.795
Size (B):   1073741824 | Time/transfer (s):     0.008071 | Bandwidth (GB/s):      133.040
```

### 2ï¸âƒ£ Submit the *IPCâ€‘disabled* job  

```bash
sbatch script_gpuaware_IPC_disabled_Olivia.sh
```

This script launches the same executable but **without** IPC support (i.e., `MPICH_GPU_IPC_ENABLED=0`).  
Typical output will show a lower bandwidth because data traverses host memory.

```
...
...
Size (B):    134217728 | Time/transfer (s):     0.017885 | Bandwidth (GB/s):        7.505
Size (B):    268435456 | Time/transfer (s):     0.036659 | Bandwidth (GB/s):        7.323
Size (B):    536870912 | Time/transfer (s):     0.074704 | Bandwidth (GB/s):        7.187
Size (B):   1073741824 | Time/transfer (s):     0.149939 | Bandwidth (GB/s):        7.161
```
### 3ï¸âƒ£ Retrieve the results  

The jobâ€™s standard output (`.out`) is stored in Slurmâ€™s `slurm_olivia_IPC_*-<jobid>.out` file in the submission directory.  
You can view it directly:

```bash
cd gpu_aware_mpi
cat slurm/slurm_olivia_IPC_enabled*.out
cat slurm/slurm_olivia_IPC_disabled*.out
```

And to view the status of your job:

```bash
squeue --me 
```

